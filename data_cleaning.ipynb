{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Datasets: bonefractureyolo and FracAtlas\n",
    "We combine images and labels from both `bonefractureyolo` and `FracAtlas` folders into a new unified dataset. This includes copying images and merging label files into a single format (YOLO or CSV as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined images: 8231\n",
      "Combined labels: 8232\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "\n",
    "# Paths for both datasets\n",
    "yolo_img_dirs = [\n",
    "    'data/bonefractureyolo/train/images',\n",
    "    'data/bonefractureyolo/valid/images',\n",
    "    'data/bonefractureyolo/test/images',\n",
    "    'data/FracAtlas/images/Fractured',\n",
    "    'data/FracAtlas/images/Non_fractured'\n",
    "# Add more if needed\n",
    " ]\n",
    "yolo_label_dirs = [\n",
    "    'data/bonefractureyolo/train/labels',\n",
    "    'data/bonefractureyolo/valid/labels',\n",
    "    'data/bonefractureyolo/test/labels',\n",
    "    # FracAtlas YOLO labels (if available)\n",
    "    'data/FracAtlas/Annotations/YOLO'\n",
    "# Add more if needed\n",
    " ]\n",
    "\n",
    "combined_img_dir = 'data/combined/images'\n",
    "combined_label_dir = 'data/combined/labels'\n",
    "\n",
    "os.makedirs(combined_img_dir, exist_ok=True)\n",
    "os.makedirs(combined_label_dir, exist_ok=True)\n",
    "\n",
    "# Copy images\n",
    "for img_dir in yolo_img_dirs:\n",
    "    if os.path.exists(img_dir):\n",
    "        for img_path in glob(os.path.join(img_dir, '*')):\n",
    "            shutil.copy(img_path, combined_img_dir)\n",
    "\n",
    "# Copy YOLO label files\n",
    "for label_dir in yolo_label_dirs:\n",
    "    if os.path.exists(label_dir):\n",
    "        for label_path in glob(os.path.join(label_dir, '*.txt')):\n",
    "            shutil.copy(label_path, combined_label_dir)\n",
    "\n",
    "print(f\"Combined images: {len(os.listdir(combined_img_dir))}\")\n",
    "print(f\"Combined labels: {len(os.listdir(combined_label_dir))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fractured: 8\n",
      "Training non-fractured: 6576\n",
      "Validation fractured: 3\n",
      "Validation non-fractured: 1644\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create train/val structure for combined dataset\n",
    "combined_train_dir = 'data/combined_organized/train'\n",
    "combined_val_dir = 'data/combined_organized/val'\n",
    "\n",
    "# Create class directories\n",
    "for split in ['train', 'val']:\n",
    "    for class_name in ['fractured', 'non_fractured']:\n",
    "        os.makedirs(f'data/combined_organized/{split}/{class_name}', exist_ok=True)\n",
    "\n",
    "# Get all images from combined directory\n",
    "all_images = [f for f in os.listdir(combined_img_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "# Classify images based on source or naming patterns\n",
    "fractured_images = []\n",
    "non_fractured_images = []\n",
    "\n",
    "for img in all_images:\n",
    "    # Classify based on source directory or filename patterns\n",
    "    if 'fracture' in img.lower() or 'fractured' in img.lower():\n",
    "        fractured_images.append(img)\n",
    "    else:\n",
    "        non_fractured_images.append(img)\n",
    "\n",
    "# If we can't classify by name, check original source directories\n",
    "for img in all_images:\n",
    "    if img not in fractured_images and img not in non_fractured_images:\n",
    "        # Default classification - you may need to adjust this logic\n",
    "        non_fractured_images.append(img)\n",
    "\n",
    "# Split each class into train/val (80/20 split)\n",
    "frac_train, frac_val = train_test_split(fractured_images, test_size=0.2, random_state=42)\n",
    "non_frac_train, non_frac_val = train_test_split(non_fractured_images, test_size=0.2, random_state=42)\n",
    "\n",
    "# Copy images to organized structure\n",
    "def copy_images_to_split(image_list, source_dir, dest_dir):\n",
    "    for img in image_list:\n",
    "        src_path = os.path.join(source_dir, img)\n",
    "        if os.path.exists(src_path):\n",
    "            shutil.copy(src_path, dest_dir)\n",
    "\n",
    "# Copy fractured images\n",
    "copy_images_to_split(frac_train, combined_img_dir, f'{combined_train_dir}/fractured')\n",
    "copy_images_to_split(frac_val, combined_img_dir, f'{combined_val_dir}/fractured')\n",
    "\n",
    "# Copy non-fractured images\n",
    "copy_images_to_split(non_frac_train, combined_img_dir, f'{combined_train_dir}/non_fractured')\n",
    "copy_images_to_split(non_frac_val, combined_img_dir, f'{combined_val_dir}/non_fractured')\n",
    "\n",
    "print(f\"Training fractured: {len(os.listdir(f'{combined_train_dir}/fractured'))}\")\n",
    "print(f\"Training non-fractured: {len(os.listdir(f'{combined_train_dir}/non_fractured'))}\")\n",
    "print(f\"Validation fractured: {len(os.listdir(f'{combined_val_dir}/fractured'))}\")\n",
    "print(f\"Validation non-fractured: {len(os.listdir(f'{combined_val_dir}/non_fractured'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated Data Pipeline for Combined Dataset\n",
    "The code above combines images from both datasets and organizes them into a standard ImageFolder structure:\n",
    "- `data/combined_organized/train/fractured/` - Training images with fractures\n",
    "- `data/combined_organized/train/non_fractured/` - Training images without fractures\n",
    "- `data/combined_organized/val/fractured/` - Validation images with fractures\n",
    "- `data/combined_organized/val/non_fractured/` - Validation images without fractures\n",
    "\n",
    "This unified structure allows us to use PyTorch's ImageFolder dataset loader efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2 & 3: Data Preprocessing, Cleaning, and Baseline Model\n",
    "- **Week 2:** Data normalization, augmentation, and cleaning are performed above using torchvision transforms and DataLoader setup.\n",
    "- **Week 3:** Develop a simple baseline CNN for bone fracture detection. The following cells define, train, and validate a basic convolutional neural network using the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original fractured: 8\n",
      "Original non-fractured: 6576\n",
      "Final dataset size: 8548\n",
      "Final fractured: 1972\n",
      "Final non-fractured: 6576\n",
      "Found 1647 image files in data/combined_organized/val\n",
      "Batch image tensor shape: torch.Size([32, 3, 224, 224])\n",
      "Batch label tensor shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define preprocessing and augmentation transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),               # resize to match model input\n",
    "    transforms.RandomHorizontalFlip(p=0.5),      # flip horizontally\n",
    "    transforms.RandomRotation(degrees=10),       # small random rotations\n",
    "    transforms.ColorJitter(brightness=0.2,       # random brightness changes\n",
    "                           contrast=0.2),        \n",
    "    transforms.ToTensor(),                       # convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],   # normalize (ImageNet mean/std)\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation/Test transforms (no augmentation, only normalization)\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# Load datasets using balanced loader for training\n",
    "train_dataset = BalancedImageFolder(root=\"data/combined_organized/train\", transform=train_transforms, oversample_ratio=0.3)\n",
    "val_dataset = LenientImageFolder(root=\"data/combined_organized/val\", transform=val_transforms)\n",
    "\n",
    "# Data loaders (reduced num_workers to avoid OS errors on macOS)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "# Inspect one batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(\"Batch image tensor shape:\", images.shape)\n",
    "print(\"Batch label tensor shape:\", labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenient dataset loader created!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageFile\n",
    "import torch.utils.data as data\n",
    "from torchvision.datasets.folder import default_loader\n",
    "\n",
    "# Enable loading of truncated images - this allows PIL to load partially corrupted images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Custom dataset class that is very lenient with image loading\n",
    "class LenientImageFolder(data.Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.classes = ['fractured', 'non_fractured']\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        \n",
    "        # Collect all image paths (don't pre-validate to be more lenient)\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root, class_name)\n",
    "            if os.path.exists(class_dir):\n",
    "                for img_name in os.listdir(class_dir):\n",
    "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        img_path = os.path.join(class_dir, img_name)\n",
    "                        self.samples.append((img_path, self.class_to_idx[class_name]))\n",
    "                            \n",
    "        print(f\"Found {len(self.samples)} image files in {root}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        try:\n",
    "            # Try to open and convert the image\n",
    "            with Image.open(path) as img:\n",
    "                sample = img.convert('RGB')\n",
    "                if self.transform is not None:\n",
    "                    sample = self.transform(sample)\n",
    "                return sample, target\n",
    "        except Exception as e:\n",
    "            # If loading fails, create a black placeholder image\n",
    "            sample = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "            if self.transform is not None:\n",
    "                sample = self.transform(sample)\n",
    "            return sample, target\n",
    "\n",
    "print(\"Lenient dataset loader created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset loader created!\n"
     ]
    }
   ],
   "source": [
    "# Create a more balanced dataset by oversampling fractured images\n",
    "import random\n",
    "\n",
    "class BalancedImageFolder(data.Dataset):\n",
    "    def __init__(self, root, transform=None, oversample_ratio=0.1):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.classes = ['fractured', 'non_fractured']\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        \n",
    "        # Collect all image paths by class\n",
    "        fractured_samples = []\n",
    "        non_fractured_samples = []\n",
    "        \n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root, class_name)\n",
    "            if os.path.exists(class_dir):\n",
    "                for img_name in os.listdir(class_dir):\n",
    "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        img_path = os.path.join(class_dir, img_name)\n",
    "                        sample = (img_path, self.class_to_idx[class_name])\n",
    "                        if class_name == 'fractured':\n",
    "                            fractured_samples.append(sample)\n",
    "                        else:\n",
    "                            non_fractured_samples.append(sample)\n",
    "        \n",
    "        # Calculate how many times to repeat fractured images\n",
    "        target_fractured_count = int(len(non_fractured_samples) * oversample_ratio)\n",
    "        oversample_factor = max(1, target_fractured_count // len(fractured_samples)) if fractured_samples else 1\n",
    "        \n",
    "        # Add samples to dataset\n",
    "        self.samples = non_fractured_samples.copy()\n",
    "        \n",
    "        # Oversample fractured images\n",
    "        for _ in range(oversample_factor):\n",
    "            self.samples.extend(fractured_samples)\n",
    "        \n",
    "        # Add some random additional fractured samples if needed\n",
    "        remaining = target_fractured_count - (len(fractured_samples) * oversample_factor)\n",
    "        if remaining > 0 and fractured_samples:\n",
    "            additional_samples = random.sample(fractured_samples, min(remaining, len(fractured_samples)))\n",
    "            self.samples.extend(additional_samples)\n",
    "        \n",
    "        # Shuffle the dataset\n",
    "        random.shuffle(self.samples)\n",
    "        \n",
    "        print(f\"Original fractured: {len(fractured_samples)}\")\n",
    "        print(f\"Original non-fractured: {len(non_fractured_samples)}\")\n",
    "        print(f\"Final dataset size: {len(self.samples)}\")\n",
    "        \n",
    "        # Count final class distribution\n",
    "        final_counts = {0: 0, 1: 0}\n",
    "        for _, label in self.samples:\n",
    "            final_counts[label] += 1\n",
    "        print(f\"Final fractured: {final_counts[0]}\")\n",
    "        print(f\"Final non-fractured: {final_counts[1]}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        try:\n",
    "            with Image.open(path) as img:\n",
    "                sample = img.convert('RGB')\n",
    "                if self.transform is not None:\n",
    "                    sample = self.transform(sample)\n",
    "                return sample, target\n",
    "        except Exception as e:\n",
    "            # If loading fails, create a black placeholder image\n",
    "            sample = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "            if self.transform is not None:\n",
    "                sample = self.transform(sample)\n",
    "            return sample, target\n",
    "\n",
    "print(\"Balanced dataset loader created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=100352, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Simple CNN baseline model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 56 * 56, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Determine number of classes from dataset\n",
    "num_classes = len(train_dataset.classes)\n",
    "model = SimpleCNN(num_classes=num_classes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Training Loss: 0.0895\n",
      "Validation Accuracy: 99.21%\n",
      "Validation Accuracy: 99.21%\n",
      "Epoch 2/5, Training Loss: 0.0043\n",
      "Epoch 2/5, Training Loss: 0.0043\n",
      "Validation Accuracy: 99.88%\n",
      "Validation Accuracy: 99.88%\n",
      "Epoch 3/5, Training Loss: 0.0057\n",
      "Epoch 3/5, Training Loss: 0.0057\n",
      "Validation Accuracy: 99.39%\n",
      "Validation Accuracy: 99.39%\n",
      "Epoch 4/5, Training Loss: 0.0052\n",
      "Epoch 4/5, Training Loss: 0.0052\n",
      "Validation Accuracy: 99.88%\n",
      "Validation Accuracy: 99.88%\n",
      "Epoch 5/5, Training Loss: 0.0001\n",
      "Epoch 5/5, Training Loss: 0.0001\n",
      "Validation Accuracy: 99.82%\n",
      "Validation Accuracy: 99.82%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Training settings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5  # For demonstration, increase as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_acc = 100 * correct / total\n",
    "    print(f'Validation Accuracy: {val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: Counter({1: 6576, 0: 1972})\n",
      "Class weights: tensor([2.1673, 0.6499])\n",
      "Epoch 1/10:\n",
      "  Training Loss: 0.0164, Training Acc: 99.66%\n",
      "  Validation Acc: 99.70%\n",
      "Epoch 1/10:\n",
      "  Training Loss: 0.0164, Training Acc: 99.66%\n",
      "  Validation Acc: 99.70%\n",
      "Epoch 2/10:\n",
      "  Training Loss: 0.0073, Training Acc: 99.80%\n",
      "  Validation Acc: 99.57%\n",
      "Epoch 2/10:\n",
      "  Training Loss: 0.0073, Training Acc: 99.80%\n",
      "  Validation Acc: 99.57%\n",
      "Epoch 3/10:\n",
      "  Training Loss: 0.0001, Training Acc: 100.00%\n",
      "  Validation Acc: 99.76%\n",
      "Epoch 3/10:\n",
      "  Training Loss: 0.0001, Training Acc: 100.00%\n",
      "  Validation Acc: 99.76%\n",
      "Epoch 4/10:\n",
      "  Training Loss: 0.0000, Training Acc: 100.00%\n",
      "  Validation Acc: 99.76%\n",
      "Epoch 4/10:\n",
      "  Training Loss: 0.0000, Training Acc: 100.00%\n",
      "  Validation Acc: 99.76%\n",
      "Epoch 5/10:\n",
      "  Training Loss: 0.0029, Training Acc: 99.88%\n",
      "  Validation Acc: 99.09%\n",
      "\n",
      "Detailed metrics for epoch 5:\n",
      "Confusion Matrix:\n",
      "[[   2    1]\n",
      " [  14 1630]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    fractured       0.12      0.67      0.21         3\n",
      "non_fractured       1.00      0.99      1.00      1644\n",
      "\n",
      "     accuracy                           0.99      1647\n",
      "    macro avg       0.56      0.83      0.60      1647\n",
      " weighted avg       1.00      0.99      0.99      1647\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 5/10:\n",
      "  Training Loss: 0.0029, Training Acc: 99.88%\n",
      "  Validation Acc: 99.09%\n",
      "\n",
      "Detailed metrics for epoch 5:\n",
      "Confusion Matrix:\n",
      "[[   2    1]\n",
      " [  14 1630]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    fractured       0.12      0.67      0.21         3\n",
      "non_fractured       1.00      0.99      1.00      1644\n",
      "\n",
      "     accuracy                           0.99      1647\n",
      "    macro avg       0.56      0.83      0.60      1647\n",
      " weighted avg       1.00      0.99      0.99      1647\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 6/10:\n",
      "  Training Loss: 0.0289, Training Acc: 99.52%\n",
      "  Validation Acc: 99.88%\n",
      "Epoch 6/10:\n",
      "  Training Loss: 0.0289, Training Acc: 99.52%\n",
      "  Validation Acc: 99.88%\n",
      "Epoch 7/10:\n",
      "  Training Loss: 0.0011, Training Acc: 99.95%\n",
      "  Validation Acc: 99.82%\n",
      "Epoch 7/10:\n",
      "  Training Loss: 0.0011, Training Acc: 99.95%\n",
      "  Validation Acc: 99.82%\n",
      "Epoch 8/10:\n",
      "  Training Loss: 0.0000, Training Acc: 100.00%\n",
      "  Validation Acc: 99.88%\n",
      "Epoch 8/10:\n",
      "  Training Loss: 0.0000, Training Acc: 100.00%\n",
      "  Validation Acc: 99.88%\n",
      "Epoch 9/10:\n",
      "  Training Loss: 0.0007, Training Acc: 99.99%\n",
      "  Validation Acc: 99.82%\n",
      "Epoch 9/10:\n",
      "  Training Loss: 0.0007, Training Acc: 99.99%\n",
      "  Validation Acc: 99.82%\n",
      "Epoch 10/10:\n",
      "  Training Loss: 0.0000, Training Acc: 100.00%\n",
      "  Validation Acc: 99.88%\n",
      "\n",
      "Detailed metrics for epoch 10:\n",
      "Confusion Matrix:\n",
      "[[   2    1]\n",
      " [   1 1643]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    fractured       0.67      0.67      0.67         3\n",
      "non_fractured       1.00      1.00      1.00      1644\n",
      "\n",
      "     accuracy                           1.00      1647\n",
      "    macro avg       0.83      0.83      0.83      1647\n",
      " weighted avg       1.00      1.00      1.00      1647\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 10/10:\n",
      "  Training Loss: 0.0000, Training Acc: 100.00%\n",
      "  Validation Acc: 99.88%\n",
      "\n",
      "Detailed metrics for epoch 10:\n",
      "Confusion Matrix:\n",
      "[[   2    1]\n",
      " [   1 1643]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    fractured       0.67      0.67      0.67         3\n",
      "non_fractured       1.00      1.00      1.00      1644\n",
      "\n",
      "     accuracy                           1.00      1647\n",
      "    macro avg       0.83      0.83      0.83      1647\n",
      " weighted avg       1.00      1.00      1.00      1647\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "# Calculate class weights to handle imbalance\n",
    "train_labels = [train_dataset.samples[i][1] for i in range(len(train_dataset))]\n",
    "class_counts = Counter(train_labels)\n",
    "total_samples = len(train_labels)\n",
    "\n",
    "# Calculate weights inversely proportional to class frequency\n",
    "class_weights = []\n",
    "for i in range(len(train_dataset.classes)):\n",
    "    weight = total_samples / (len(train_dataset.classes) * class_counts[i])\n",
    "    class_weights.append(weight)\n",
    "\n",
    "class_weights = torch.FloatTensor(class_weights)\n",
    "print(f\"Class distribution: {class_counts}\")\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Training settings with weighted loss\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# Use weighted CrossEntropyLoss to handle class imbalance\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10  # Increased epochs for better training\n",
    "\n",
    "# Training loop with detailed metrics\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    train_acc = 100 * train_correct / train_total\n",
    "    \n",
    "    # Validation with detailed metrics\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_predicted = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_predicted.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "    print(f'  Training Loss: {epoch_loss:.4f}, Training Acc: {train_acc:.2f}%')\n",
    "    print(f'  Validation Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    # Print detailed metrics every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'\\nDetailed metrics for epoch {epoch+1}:')\n",
    "        print('Confusion Matrix:')\n",
    "        cm = confusion_matrix(all_labels, all_predicted)\n",
    "        print(cm)\n",
    "        print('\\nClassification Report:')\n",
    "        print(classification_report(all_labels, all_predicted, \n",
    "                                  target_names=train_dataset.classes))\n",
    "        print('-' * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicum2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
